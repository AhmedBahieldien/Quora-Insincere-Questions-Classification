{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow.compat.v1 as tf\nimport tensorflow_hub as hub\nimport contractions\nfrom bs4 import BeautifulSoup\nimport unicodedata\nimport re\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-hub\n!pip install contractions\n!pip install beautifulsoup4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = dataset['question_text'].values\ny = dataset['target'].values\n\ntrain_x = x[:30000]\ntrain_y = y[:30000]\n\nval_x = x[30000:35000]\nval_y = y[30000:35000]\n\ntest_x = x[35000:]\ntest_y = y[35000:]\ntrain_x.shape, val_x.shape, test_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    [s.extract() for s in soup(['iframe', 'script'])]\n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n    return stripped_text\n\n\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n\ndef expand_contractions(text):\n    return contractions.fix(text)\n\n\n\ndef remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\n\ndef pre_process_document(document):\n    \n    document = strip_html_tags(document)\n    document = document.lower()\n    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \")) \n    document = remove_accented_chars(document)   \n    document = expand_contractions(document)   \n    special_char_pattern = re.compile(r'([{.(-)!}])')\n    document = special_char_pattern.sub(\" \\\\1 \", document)\n    document = remove_special_characters(document, remove_digits=True)  \n    document = re.sub(' +', ' ', document)\n    document = document.strip()\n    \n    return document\n\n\npre_process_corpus = np.vectorize(pre_process_document)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = pre_process_corpus(train_x)\nval_x = pre_process_corpus(val_x)\ntest_x = pre_process_corpus(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input_fn = tf.estimator.inputs.numpy_input_fn(\n    {'sentence': train_x}, train_y, \n    batch_size=256, num_epochs=None, shuffle=True)\n\npredict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n    {'sentence': train_x}, train_y, shuffle=False)\n\npredict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n    {'sentence': val_x}, val_y, shuffle=False)\n\npredict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n    {'sentence': test_x}, test_y, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_feature = hub.text_embedding_column(\n    key='sentence', \n    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/2\",\n    trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dnn = tf.estimator.DNNClassifier(\n          hidden_units=[512, 128],\n          feature_columns=[embedding_feature],\n          n_classes=2,\n          activation_fn=tf.nn.relu,\n          dropout=0.1,\n          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.logging.set_verbosity(tf.logging.ERROR)\n\nTOTAL_STEPS = 1000\nSTEP_SIZE = 100\nfor step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n    print()\n    print('-'*100)\n    print('Training for step =', step)\n    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}